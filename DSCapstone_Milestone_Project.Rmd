---
title: "<span style='font-size: 48px'>Capstone - Milestone Report</span>"
author: "<span style='font-size: 24px'>Henry CY Wong</span>"
date: "<span style='font-size: 24px'>2021-07-18</span>"
output:
    html_document:
            css: style.css
            keep_md: true
---

&nbsp;
&nbsp;

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

This milestone project is used to illustrate the exploratory data analysis of the given SwiftKey dataset as a preparation of the Shiny App and algorithm of the Capstone Project.

&nbsp;
&nbsp;

## 2. Libraries Used

&nbsp;

The following libraries will be used:

```{r load_library, warning=FALSE, error=FALSE, message=FALSE}
library(stringi)
library(tm)
library(RWeka)
library(ggplot2)
```

&nbsp;
&nbsp;


## 3. Data Preparation

The dataset will be downloaded from [SwiftKey](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).
There are three data files included which are

- **en_US.blogs.txt**
- **en_US.news.txt**
- **en_US.twitter.txt** 

```{r download_dataset, echo=FALSE}
data_url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
data_file <- "Coursera-SwiftKey.zip"
data_dir <- "./data"

if (!file.exists(data_dir)) {
    dir.create(data_dir)
    }

if (!file.exists(data_file)) {
    download.file(data_url, destfile = data_file, method = "curl")
    unzip(data_file, exdir = data_dir)
}

```

```{r read_files, echo=FALSE}

blogs_file <- "./data/final/en_US/en_US.blogs.txt"
news_file <- "./data/final/en_US/en_US.news.txt"
twitter_file <- "./data/final/en_US/en_US.twitter.txt" 

# read files
blogs <- readLines(blogs_file, encoding = "UTF-8", skipNul = TRUE)
news <- readLines(news_file, encoding = "UTF-8", skipNul = TRUE)
twitter <- readLines(twitter_file, encoding = "UTF-8", skipNul = TRUE)

# file sizes in Mb
blogs_size <- file.size(blogs_file)/(1024^2)
news_size <- file.size(news_file)/(1024^2)
twitter_size <- file.size(twitter_file)/(1024^2)

# number of lines per files
blogs_lines <- length(blogs)
news_lines <- length(news)
twitter_lines <- length(twitter)

# words per files
blogs_words <- stri_stats_latex(blogs)[4]
news_words <- stri_stats_latex(news)[4]
twitter_words <- stri_stats_latex(twitter)[4]

# file summary
file_summary <- data.frame(file_name = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"),
                           file_size_in_Mb = c(blogs_size, news_size, twitter_size),
                           lines_per_file = c(blogs_lines, news_lines, twitter_lines),
                           words_per_file = c(blogs_words, news_words, twitter_words))

```

The summary of given files are as shown below:
```{r show_file_summary, echo=FALSE}
file_summary
```

&nbsp;
&nbsp;

## 4. Data Cleaning

Since the size of data is huge, in order to reduce the processing time, a smaller dataset is considereed. Hence, only 1% of the number of lines per file will be extracted as samples for analysis. Samples of three given files will be combined into one sample dataset for cleaning and analysis. The number of lines of the sample sample dataset will be 

```{r make_sample_set, echo=FALSE}
sample_size <- 0.01
set.seed(20210718)
blogs_samples <- sample(blogs, size = blogs_lines * sample_size, replace = TRUE)
news_samples <- sample(news, size = news_lines * sample_size, replace = TRUE)
twitter_samples <- sample(twitter, size = twitter_lines * sample_size, replace = TRUE)
my_samples <- c(blogs_samples, news_samples, twitter_samples)
length(my_samples)
```

Then, the sample dataset will be converted into a corpus and the following characters or words will be will cleaned or transformed:

* Remove URL
* Remove Twitter Handler
* Remove Email Address
* Remove Profanity Words
* Transform all words to lowercase
* Remove English Stopwords
* Remove Numbers
* Remove Punctuation

**Credit: The Profanity Word List is downloaded from [Luis von Ahn's](https://www.cs.cmu.edu/~biglou/resources/) for corresponding profanity checking and removal.**

```{r download_profanity_list, echo=FALSE}
profanity_url <- "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
profanity_file <- "./data/bad-words.txt"
data_dir <- "./data"

if (!file.exists(data_dir)) {
    dir.create(data_dir)
    }

if (!file.exists(profanity_file)) {
    download.file(profanity_url, destfile = profanity_file, method = "curl")
    }

profanity_words <- readLines(profanity_file, encoding = "UTF-8", skipNul = TRUE)
```

```{r data_clean, echo=FALSE}

my_corpus <- VCorpus(VectorSource(my_samples))

to_space <- content_transformer(function(in_str, my_pattern) gsub(my_pattern, " ", in_str))

url_pattern <- "(f|ht)tp(s?)://(.*)[.][a-z]+"
twitter_pattern <- "@[^\\s]+"
email_pattern <- "\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b"

my_corpus <- tm_map(my_corpus, to_space, url_pattern)
my_corpus <- tm_map(my_corpus, to_space, twitter_pattern)
my_corpus <- tm_map(my_corpus, to_space, email_pattern)

my_corpus <- tm_map(my_corpus, removeWords, profanity_words)
my_corpus <- tm_map(my_corpus, tolower)
my_corpus <- tm_map(my_corpus, removePunctuation)
my_corpus <- tm_map(my_corpus, removeNumbers)
my_corpus <- tm_map(my_corpus, stripWhitespace)
my_corpus <- tm_map(my_corpus, removeWords, stopwords("en"))
my_corpus <- tm_map(my_corpus, PlainTextDocument)

```

&nbsp;

The sample after cleaning will look like:

```{r show_samples, echo=FALSE}
my_corpus_df <- data.frame(text = unlist(sapply(my_corpus, '[', "content")), stringsAsFactors = FALSE)
head(my_corpus_df)
```

&nbsp;
&nbsp;

## 5. Exploratory Data Analysis


&nbsp;
&nbsp;


### 5.1 Unigram

```{r show_unigram, echo=FALSE}

unigram <- NGramTokenizer(my_corpus_df, Weka_control(min = 1, max = 1))
unigram <- data.frame(table(unigram))
unigram <- unigram[order(unigram$Freq, decreasing = TRUE),]
names(unigram) <- c("one_word", "freq")

g_unigram <- ggplot(data = unigram[1:20,], aes(x=reorder(one_word, freq), y=freq))
g_unigram <- g_unigram + geom_bar(stat="identity") + coord_flip()
g_unigram <- g_unigram + xlab("") + ylab("Frequency") + ggtitle("Frequency of 20 Most Common Unigrams")
g_unigram

```

&nbsp;
&nbsp;

### 5.2 Bigram

```{r show_bigram, echo=FALSE}

bigram <- NGramTokenizer(my_corpus_df, Weka_control(min = 2, max = 2))
bigram <- data.frame(table(bigram))
bigram <- bigram[order(bigram$Freq, decreasing = TRUE),]
names(bigram) <- c("two_words", "freq")

g_bigram <- ggplot(data = bigram[1:20,], aes(x=reorder(two_words, freq), y=freq))
g_bigram <- g_bigram + geom_bar(stat="identity") + coord_flip()
g_bigram <- g_bigram + xlab("") + ylab("Frequency") + ggtitle("Frequency of 20 Most Common Bigrams")
g_bigram

```
&nbsp;
&nbsp;


### 5.3 Trigram

```{r show_trigram, echo=FALSE}

trigram <- NGramTokenizer(my_corpus_df, Weka_control(min = 3, max = 3))
trigram <- data.frame(table(trigram))
trigram <- trigram[order(trigram$Freq, decreasing = TRUE),]
names(trigram) <- c("three_words", "freq")

g_trigram <- ggplot(data = trigram[1:20,], aes(x=reorder(three_words, freq), y=freq))
g_trigram <- g_trigram + geom_bar(stat="identity") + coord_flip()
g_trigram <- g_trigram + xlab("") + ylab("Frequency") + ggtitle("Frequency of 20 Most Common Trigrams")
g_trigram

```

&nbsp;
&nbsp;

## 6. Next Steps

&nbsp;
&nbsp;

